{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oJHO4EZxish"
      },
      "source": [
        "# ΕΠ08 Αναγνώριση Προτύπων – Μηχανική Μάθηση 2η Εργασία\n",
        "Όνομα: Μανίκα Θεοδώρα\n",
        "\n",
        "ΑΜ: 1115202100267"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjY7ls444Zs4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
        "import time\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "!pip install yt-dlp pydub\n",
        "\n",
        "import yt_dlp\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy.stats import mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElbppcrEwLPV"
      },
      "source": [
        "# Ερώτημα 1: Feedforward Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmqS1odQwOsB"
      },
      "source": [
        "## Βήμα 1: Φόρτωση δεδομένων (mfccs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etuamvezi1c8"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/music_genre_data_di\"\n",
        "os.chdir(data_path)\n",
        "\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "print(\"\\nContents of the directory:\")\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DKExgbGzYo8"
      },
      "outputs": [],
      "source": [
        "data_paths = {\n",
        "    'train': 'train/pyaudioanalysis/',\n",
        "    'val': 'val/pyaudioanalysis/',\n",
        "    'test': 'test/pyaudioanalysis/'\n",
        "}\n",
        "\n",
        "# Initialize dictionaries to store the data\n",
        "X_data = {}\n",
        "y_data = {}\n",
        "\n",
        "# Load data\n",
        "for dataset, path in data_paths.items():\n",
        "    X_data[dataset] = np.load(os.path.join(path, 'X.npy'))\n",
        "    y_data[dataset] = np.load(os.path.join(path, 'labels.npy'))\n",
        "\n",
        "X_train, y_train = X_data['train'], y_data['train']\n",
        "X_val, y_val = X_data['val'], y_data['val']\n",
        "X_test, y_test = X_data['test'], y_data['test']\n",
        "\n",
        "# Convert labels from strings to integers (0-3)\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_val_encoded = label_encoder.transform(y_val)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Create mapping dictionary for reference\n",
        "class_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
        "print(\"Class mapping:\", class_mapping)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjA5-GBU1Fcx"
      },
      "outputs": [],
      "source": [
        "# Convert numpy arrays to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train)\n",
        "y_train_tensor = torch.LongTensor(y_train_encoded)\n",
        "X_val_tensor = torch.FloatTensor(X_val)\n",
        "y_val_tensor = torch.LongTensor(y_val_encoded)\n",
        "X_test_tensor = torch.FloatTensor(X_test)\n",
        "y_test_tensor = torch.LongTensor(y_test_encoded)\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Create DataLoaders with batch size 16\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZuMQe7l1Jqa"
      },
      "outputs": [],
      "source": [
        "# Print information about the loaded data\n",
        "print(\"\\nData loading complete!\")\n",
        "print(f\"Number of training batches: {len(train_loader)}\")\n",
        "print(f\"Number of validation batches: {len(val_loader)}\")\n",
        "print(f\"Number of test batches: {len(test_loader)}\")\n",
        "\n",
        "# Check the shape of one batch\n",
        "sample_features, sample_labels = next(iter(train_loader))\n",
        "print(f\"\\nSample batch features shape: {sample_features.shape}\")\n",
        "print(f\"Sample batch labels shape: {sample_labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsNJWUvBwSEr"
      },
      "source": [
        "## Βήμα 2: Ορισμός Νευρωνικού Δικτύου"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot5ZVm9G1ntO"
      },
      "outputs": [],
      "source": [
        "class FCNN(nn.Module):\n",
        "    def __init__(self, input_dim=26):\n",
        "        \"\"\"\n",
        "        Fully Connected Neural Network with 3 layers:\n",
        "        - Input layer: input_dim (26) → 128 neurons\n",
        "        - Hidden layer: 128 → 32 neurons\n",
        "        - Output layer: 32 → 4 neurons (output classes)\n",
        "\n",
        "        No activation functions are used as specified.\n",
        "        \"\"\"\n",
        "        super(FCNN, self).__init__()\n",
        "\n",
        "        # Define the network layers\n",
        "        self.layer1 = nn.Linear(input_dim, 128)  # Input to first hidden layer\n",
        "        self.layer2 = nn.Linear(128, 32)         # First to second hidden layer\n",
        "        self.layer3 = nn.Linear(32, 4)           # Second hidden to output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the network\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, 26)\n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, 4)\n",
        "        \"\"\"\n",
        "        # Pass through each layer without activation functions\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5qsXGYewVIg"
      },
      "source": [
        "## Βήμα 3: Ορισμός διαδικασίας εκπαίδευσης"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csnUcuQ22WBS"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    \"\"\"\n",
        "    Training function for the neural network\n",
        "\n",
        "    Args:\n",
        "        model: The neural network to train\n",
        "        train_loader: DataLoader with training data\n",
        "        criterion: Loss function\n",
        "        optimizer: Optimization algorithm\n",
        "        num_epochs: Number of training epochs\n",
        "\n",
        "    Returns:\n",
        "        The trained model\n",
        "    \"\"\"\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print statistics\n",
        "            running_loss += loss.item()\n",
        "            if batch_idx % 100 == 99:  # Print every 100 batches\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], '\n",
        "                      f'Loss: {running_loss/100:.4f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # Print epoch statistics\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
        "\n",
        "    print('Training complete!')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blXq688PwYLZ"
      },
      "source": [
        "## Βήμα 4: Ορισμός διαδικασίας αξιολόγησης"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8KkAFS956Cp"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_device(model, dataloader, criterion, device): # Added device argument\n",
        "    \"\"\"\n",
        "    Evaluation function for the neural network\n",
        "\n",
        "    Args:\n",
        "        model: The neural network to evaluate\n",
        "        dataloader: DataLoader with evaluation data\n",
        "        criterion: Loss function\n",
        "        device: The device to perform evaluation on ('cuda' or 'cpu')\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - loss (float): Average loss across all batches\n",
        "        - f1 (float): Macro averaged F1 score\n",
        "        - accuracy (float): Accuracy score\n",
        "        - cm (numpy array): Confusion matrix\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for inputs, labels in dataloader:\n",
        "            # Move inputs and labels to the specified device\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Store predictions and labels for metrics calculation\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "    return avg_loss, f1, accuracy, cm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xjH9mRSwbZ9"
      },
      "source": [
        "## Βήμα 5: Εκπαίδευση δικτύου"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6gbnW4r6WoM"
      },
      "outputs": [],
      "source": [
        "# Εκπαίδευση με CPU για σύγκριση\n",
        "# Ensure we use the CPU device for this part\n",
        "device_cpu = torch.device('cpu')\n",
        "print(f\"Using device: {device_cpu}\")\n",
        "model_cpu_fcnn = FCNN(input_dim=26).to(device_cpu) # Move model to CPU\n",
        "optimizer_cpu = optim.SGD(model_cpu_fcnn.parameters(), lr=0.002)\n",
        "criterion_cpu = nn.CrossEntropyLoss() # Create a separate criterion if needed, or reuse\n",
        "\n",
        "print(\"\\nTraining with CPU...\")\n",
        "start_time_cpu = time.time()\n",
        "num_epochs = 30\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_cpu_fcnn.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "        # Move inputs and labels to the CPU device\n",
        "        inputs, labels = inputs.to(device_cpu), labels.to(device_cpu)\n",
        "\n",
        "        optimizer_cpu.zero_grad()\n",
        "        outputs = model_cpu_fcnn(inputs)\n",
        "        loss = criterion_cpu(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_cpu.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if batch_idx % 100 == 99:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Optional: Print epoch loss at the end of each epoch\n",
        "    if running_loss > 0:\n",
        "         print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Remaining Loss: {running_loss/(batch_idx % 100 + 1):.4f}')\n",
        "\n",
        "\n",
        "cpu_duration = time.time() - start_time_cpu\n",
        "print(f'CPU Training completed in {cpu_duration:.2f} seconds')\n",
        "\n",
        "# Αξιολόγηση με CPU - Pass the device_cpu to evaluate_model\n",
        "test_loss_cpu, test_f1_cpu, test_acc_cpu, test_cm_cpu = evaluate_model_device(model_cpu_fcnn, test_loader, criterion_cpu, device_cpu)\n",
        "print(\"\\nCPU Test Results:\")\n",
        "print(f\"Loss: {test_loss_cpu:.4f}\")\n",
        "print(f\"F1 Score (macro): {test_f1_cpu:.4f}\")\n",
        "print(f\"Accuracy: {test_acc_cpu:.4f}\")\n",
        "print(\"\\nConfusion Matrix (CPU):\")\n",
        "print(test_cm_cpu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYwcVGbAweCJ"
      },
      "source": [
        "## Βήμα 6: Εκπαίδευση δικτύου με GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5F1ZC-FR9ycJ"
      },
      "outputs": [],
      "source": [
        "# Έλεγχος για GPU διαθεσιμότητα\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Μεταφορά μοντέλου και δεδομένων στη GPU\n",
        "# Create a new model instance for GPU training to ensure it starts fresh\n",
        "model_gpu_fcnn = FCNN(input_dim=26).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_gpu_fcnn.parameters(), lr=0.002)\n",
        "\n",
        "# Note: We don't need a separate to_device_dataloader function.\n",
        "# We can move tensors to the device inside the training loop directly.\n",
        "\n",
        "# Εκπαίδευση με GPU\n",
        "print(\"\\nTraining with GPU...\")\n",
        "start_time_gpu = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_gpu_fcnn.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "        # Move inputs and labels to the selected device (GPU or CPU)\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_gpu_fcnn(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if batch_idx % 100 == 99:\n",
        "            # No need to calculate epoch_loss here, as we print per 100 batches\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Optional: Print epoch loss at the end of each epoch\n",
        "    # If running_loss is not zero, calculate and print the remaining average loss\n",
        "    if running_loss > 0:\n",
        "         print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Remaining Loss: {running_loss/(batch_idx % 100 + 1):.4f}')\n",
        "\n",
        "\n",
        "gpu_duration = time.time() - start_time_gpu\n",
        "print(f'GPU Training completed in {gpu_duration:.2f} seconds')\n",
        "\n",
        "\n",
        "# Αξιολόγηση με GPU - Pass the device to evaluate_model\n",
        "# The evaluate_model function itself handles moving data to the device\n",
        "test_loss_gpu, test_f1_gpu, test_acc_gpu, test_cm_gpu = evaluate_model_device(model_gpu_fcnn, test_loader, criterion, device)\n",
        "print(\"\\nGPU Test Results:\")\n",
        "print(f\"Loss: {test_loss_gpu:.4f}\")\n",
        "print(f\"F1 Score (macro): {test_f1_gpu:.4f}\")\n",
        "print(f\"Accuracy: {test_acc_gpu:.4f}\")\n",
        "print(\"\\nConfusion Matrix (GPU):\")\n",
        "print(test_cm_gpu)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTwV5PjADgHu"
      },
      "outputs": [],
      "source": [
        "# Σύγκριση χρόνων\n",
        "print(\"\\nTraining Time Comparison:\")\n",
        "print(f\"GPU Time: {gpu_duration:.2f} seconds\")\n",
        "print(f\"CPU Time: {cpu_duration:.2f} seconds\")\n",
        "\n",
        "# Only calculate speedup if a GPU was actually used (device is 'cuda')\n",
        "if device.type == 'cuda':\n",
        "    print(f\"Speedup: {cpu_duration/gpu_duration:.2f}x faster with GPU\")\n",
        "else:\n",
        "    print(\"GPU not available, cannot calculate speedup.\")\n",
        "\n",
        "# Optional: Print classification report for GPU model evaluation\n",
        "all_preds_gpu = []\n",
        "all_labels_gpu = []\n",
        "model_gpu_fcnn.eval()\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        # Move inputs and labels to the device used for GPU model\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model_gpu_fcnn(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        all_preds_gpu.extend(predicted.cpu().numpy())\n",
        "        all_labels_gpu.extend(labels.cpu().numpy())\n",
        "\n",
        "print(\"\\nClassification Report (GPU):\")\n",
        "# Assuming label_encoder is available from previous cells\n",
        "try:\n",
        "    print(classification_report(all_labels_gpu, all_preds_gpu, target_names=label_encoder.classes_))\n",
        "except NameError:\n",
        "    print(\"label_encoder not found. Cannot print classification report with class names.\")\n",
        "    print(classification_report(all_labels_gpu, all_preds_gpu))\n",
        "\n",
        "\n",
        "# Optional: Print classification report for CPU model evaluation\n",
        "all_preds_cpu = []\n",
        "all_labels_cpu = []\n",
        "model_cpu_fcnn.eval()\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        # Move inputs and labels to the device used for CPU model\n",
        "        inputs, labels = inputs.to(device_cpu), labels.to(device_cpu)\n",
        "        outputs = model_cpu_fcnn(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        all_preds_cpu.extend(predicted.cpu().numpy())\n",
        "        all_labels_cpu.extend(labels.cpu().numpy())\n",
        "\n",
        "print(\"\\nClassification Report (CPU):\")\n",
        "try:\n",
        "    print(classification_report(all_labels_cpu, all_preds_cpu, target_names=label_encoder.classes_))\n",
        "except NameError:\n",
        "    print(\"label_encoder not found. Cannot print classification report with class names.\")\n",
        "    print(classification_report(all_labels_cpu, all_preds_cpu))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S2NjvZhwjor"
      },
      "source": [
        "## Βήμα 7: Επιλογή μοντέλου"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_mS66iK-Hsr"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0, path='best_model.pth'):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_f1, model):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = val_f1\n",
        "            self.save_checkpoint(model)\n",
        "        elif val_f1 < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = val_f1\n",
        "            self.save_checkpoint(model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "\n",
        "def train_with_validation(model, train_loader, val_loader, criterion, optimizer, num_epochs=30):\n",
        "    # Determine the device of the model\n",
        "    model_device = next(model.parameters()).device\n",
        "    early_stopping = EarlyStopping(patience=5, path='best_f1_model.pth')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            # Move inputs and labels to the model's device\n",
        "            inputs, labels = inputs.to(model_device), labels.to(model_device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                # Move inputs and labels to the model's device\n",
        "                inputs, labels = inputs.to(model_device), labels.to(model_device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate metrics\n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "        val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f}')\n",
        "\n",
        "        # Early stopping check\n",
        "        early_stopping(val_f1, model)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "    # Load the best model state dict onto the current model instance (which is on the correct device)\n",
        "    model.load_state_dict(torch.load('best_f1_model.pth', map_location=model_device))\n",
        "    return model\n",
        "\n",
        "# Initialize model, criterion and optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_fcnn = FCNN(input_dim=26).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_fcnn.parameters(), lr=0.002)\n",
        "\n",
        "# Train with validation\n",
        "best_model = train_with_validation(model_fcnn, train_loader, val_loader, criterion, optimizer, num_epochs=30)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_f1, test_acc, test_cm = evaluate_model_device(best_model, test_loader, criterion, device)\n",
        "\n",
        "print(\"\\nFinal Test Results with Best Model:\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test F1 Score (macro): {test_f1:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(test_cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhKj0RLTwoZt"
      },
      "source": [
        "# Ερώτημα 2: Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAPz_ehVws1k"
      },
      "source": [
        "## Βήμα 1: Φόρτωση δεδομένων (spectrograms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNtgkP8-E2vY"
      },
      "outputs": [],
      "source": [
        "# Define dataset splits and their paths\n",
        "data_config = {\n",
        "    'train': 'train/melgrams/',\n",
        "    'val': 'val/melgrams/',\n",
        "    'test': 'test/melgrams/'\n",
        "}\n",
        "\n",
        "# Dictionary to store loaded data\n",
        "mel_data = {}\n",
        "\n",
        "# Load data for each split\n",
        "for split, path in data_config.items():\n",
        "    mel_data[f'X_{split}_mel'] = np.load(os.path.join(path, 'X.npy'))\n",
        "    mel_data[f'y_{split}_mel'] = np.load(os.path.join(path, 'labels.npy'))\n",
        "\n",
        "# Assign to variables (optional - you can also work directly with the dictionary)\n",
        "X_train_mel = mel_data['X_train_mel']\n",
        "y_train_mel = mel_data['y_train_mel']\n",
        "X_val_mel = mel_data['X_val_mel']\n",
        "y_val_mel = mel_data['y_val_mel']\n",
        "X_test_mel = mel_data['X_test_mel']\n",
        "y_test_mel = mel_data['y_test_mel']\n",
        "\n",
        "# Convert labels from strings to integers (0-3)\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_mel_encoded = label_encoder.fit_transform(y_train_mel)\n",
        "y_val_mel_encoded = label_encoder.transform(y_val_mel)\n",
        "y_test_mel_encoded = label_encoder.transform(y_test_mel)\n",
        "\n",
        "# Create mapping dictionary for reference\n",
        "class_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
        "print(\"Class mapping:\", class_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5dsHauAEnZe"
      },
      "outputs": [],
      "source": [
        "# Check the shape of the mel-spectrograms\n",
        "print(\"\\nMel-spectrogram shapes:\")\n",
        "print(f\"Train: {X_train_mel.shape}\")\n",
        "print(f\"Validation: {X_val_mel.shape}\")\n",
        "print(f\"Test: {X_test_mel.shape}\")\n",
        "\n",
        "# Visualize one random mel-spectrogram from each class\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, genre in enumerate(label_encoder.classes_):\n",
        "    # Find indices of samples from this class\n",
        "    indices = np.where(y_train_mel_encoded == i)[0]\n",
        "    # Select a random sample\n",
        "    random_idx = random.choice(indices)\n",
        "    melgram = X_train_mel[random_idx]\n",
        "\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    plt.imshow(melgram, aspect='auto', origin='lower')\n",
        "    plt.title(f\"Class: {genre}\")\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"Time frames\")\n",
        "    plt.ylabel(\"Mel-frequency bins\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIkCb660EpUz"
      },
      "outputs": [],
      "source": [
        "# Convert numpy arrays to PyTorch tensors and add channel dimension\n",
        "# Assuming mel-spectrograms have shape (n_samples, height, width)\n",
        "X_train_mel_tensor = torch.FloatTensor(X_train_mel).unsqueeze(1)  # Add channel dimension\n",
        "y_train_mel_tensor = torch.LongTensor(y_train_mel_encoded)\n",
        "X_val_mel_tensor = torch.FloatTensor(X_val_mel).unsqueeze(1)\n",
        "y_val_mel_tensor = torch.LongTensor(y_val_mel_encoded)\n",
        "X_test_mel_tensor = torch.FloatTensor(X_test_mel).unsqueeze(1)\n",
        "y_test_mel_tensor = torch.LongTensor(y_test_mel_encoded)\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_mel_dataset = TensorDataset(X_train_mel_tensor, y_train_mel_tensor)\n",
        "val_mel_dataset = TensorDataset(X_val_mel_tensor, y_val_mel_tensor)\n",
        "test_mel_dataset = TensorDataset(X_test_mel_tensor, y_test_mel_tensor)\n",
        "\n",
        "# Create DataLoaders with batch size 16\n",
        "batch_size = 16\n",
        "train_mel_loader = DataLoader(train_mel_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_mel_loader = DataLoader(val_mel_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_mel_loader = DataLoader(test_mel_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"\\nMel-spectrogram data loading complete!\")\n",
        "print(f\"Number of training batches: {len(train_mel_loader)}\")\n",
        "print(f\"Number of validation batches: {len(val_mel_loader)}\")\n",
        "print(f\"Number of test batches: {len(test_mel_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ntA5J_8wvmm"
      },
      "source": [
        "## Βήμα 2: Ορισμός Νευρωνικού Δικτύου"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Udrwo1sPFkDz"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_height=21, input_width=128, out_dim=4):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            # Layer 1: 1 input channel, 16 output channels, kernel size 5\n",
        "            nn.Conv2d(1, 16, kernel_size=5),\n",
        "            # Layer 2: 16 input channels, 32 output channels, kernel size 5\n",
        "            nn.Conv2d(16, 32, kernel_size=5),\n",
        "            # Layer 3: 32 input channels, 64 output channels, kernel size 5\n",
        "            nn.Conv2d(32, 64, kernel_size=5),\n",
        "            # Layer 4: 64 input channels, 128 output channels, kernel size 5\n",
        "            nn.Conv2d(64, 128, kernel_size=5)\n",
        "        )\n",
        "\n",
        "        # Calculate the output dimensions after convolutions\n",
        "        # We need this to determine the input size for the first fully connected layer\n",
        "        def conv_output_size(size, kernel_size=5):\n",
        "            return size - kernel_size + 1\n",
        "\n",
        "        # Apply conv_output_size twice (once for height, once for width)\n",
        "        conv_height = conv_output_size(conv_output_size(conv_output_size(conv_output_size(input_height))))\n",
        "        conv_width = conv_output_size(conv_output_size(conv_output_size(conv_output_size(input_width))))\n",
        "\n",
        "        self.conv_output_size = 128 * conv_height * conv_width\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(self.conv_output_size, 1024),\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.Linear(256, 32),\n",
        "            nn.Linear(32, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through convolutional layers\n",
        "        x = self.conv_layers(x)\n",
        "\n",
        "        # Flatten the output for the fully connected layers\n",
        "        x = x.view(-1, self.conv_output_size)\n",
        "\n",
        "        # Forward pass through fully connected layers\n",
        "        x = self.fc_layers(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYRi4lYJwySQ"
      },
      "source": [
        "## Βήμα 3: Εκπαίδευση δικτύου"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDmKZpNNGRdA"
      },
      "outputs": [],
      "source": [
        "# Initialize model, criterion and optimizer\n",
        "input_height, input_width = X_train_mel.shape[1:]\n",
        "model_cpu_cnn = CNN(input_height=input_height, input_width=input_width).to('cpu')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_cpu_cnn.parameters(), lr=0.002)\n",
        "\n",
        "# Training with validation (with timing)\n",
        "print(\"Starting training...\")\n",
        "start_train = time.time()\n",
        "\n",
        "best_model = train_with_validation(model_cpu_cnn, train_mel_loader, val_mel_loader,\n",
        "                                 criterion, optimizer, num_epochs=30)\n",
        "\n",
        "train_time = time.time() - start_train\n",
        "print(f\"\\nTraining completed in {train_time:.2f} seconds ({train_time/60:.2f} minutes)\")\n",
        "\n",
        "# Evaluation on test set (with timing)\n",
        "print(\"\\nStarting evaluation...\")\n",
        "start_eval = time.time()\n",
        "\n",
        "test_loss, test_f1, test_acc, test_cm = evaluate_model_device(best_model,\n",
        "                                                           test_mel_loader,\n",
        "                                                           criterion,\n",
        "                                                           'cpu')\n",
        "\n",
        "eval_time = time.time() - start_eval\n",
        "\n",
        "# Print results\n",
        "print(\"\\nFinal Test Results with Best Model:\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test F1 Score (macro): {test_f1:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Evaluation completed in {eval_time:.2f} seconds\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(test_cm)\n",
        "\n",
        "# Total time\n",
        "total_time_cpu = train_time + eval_time\n",
        "print(f\"\\nTotal execution time: {total_time_cpu:.2f} seconds ({total_time_cpu/60:.2f} minutes)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdWbnWtpHPRc"
      },
      "outputs": [],
      "source": [
        "# Initialize device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize model, criterion and optimizer\n",
        "input_height, input_width = X_train_mel.shape[1:]\n",
        "model_gpu_cnn = CNN(input_height=input_height, input_width=input_width).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_gpu_cnn.parameters(), lr=0.002)\n",
        "\n",
        "# Training with validation (with timing)\n",
        "print(\"\\nStarting training...\")\n",
        "start_train = time.time()\n",
        "\n",
        "best_model = train_with_validation(model_gpu_cnn, train_mel_loader, val_mel_loader,\n",
        "                                 criterion, optimizer, num_epochs=30)\n",
        "\n",
        "train_time = time.time() - start_train\n",
        "print(f\"\\nTraining completed in {train_time:.2f} seconds ({train_time/60:.2f} minutes)\")\n",
        "\n",
        "# Evaluation on test set (with timing)\n",
        "print(\"\\nStarting evaluation...\")\n",
        "start_eval = time.time()\n",
        "\n",
        "test_loss, test_f1, test_acc, test_cm = evaluate_model_device(best_model,\n",
        "                                                           test_mel_loader,\n",
        "                                                           criterion,\n",
        "                                                           device)\n",
        "\n",
        "eval_time = time.time() - start_eval\n",
        "\n",
        "# Print results\n",
        "print(\"\\nFinal Test Results with Best Model:\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test F1 Score (macro): {test_f1:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Evaluation completed in {eval_time:.2f} seconds\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(test_cm)\n",
        "\n",
        "# Total time\n",
        "total_time_gpu = train_time + eval_time\n",
        "print(f\"\\nTotal execution time: {total_time_gpu:.2f} seconds ({total_time_gpu/60:.2f} minutes)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV0u9ILSHgNV"
      },
      "outputs": [],
      "source": [
        "# Σύγκριση χρόνων\n",
        "print(\"\\nTraining Time Comparison:\")\n",
        "print(f\"GPU Time: {total_time_gpu:.2f} seconds\")\n",
        "print(f\"CPU Time: {total_time_cpu:.2f} seconds\")\n",
        "\n",
        "# Only calculate speedup if a GPU was actually used (device is 'cuda')\n",
        "if device.type == 'cuda':\n",
        "    print(f\"Speedup: {total_time_cpu/total_time_gpu:.2f}x faster with GPU\")\n",
        "else:\n",
        "    print(\"GPU not available, cannot calculate speedup.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9X9Onavw1c2"
      },
      "source": [
        "## Βήμα 4: Pooling and padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GO7ljyIII95Q"
      },
      "outputs": [],
      "source": [
        "class CNNP(nn.Module):\n",
        "    def __init__(self, input_height, input_width):\n",
        "        \"\"\"\n",
        "        Modified CNN with padding and max pooling for mel-spectrogram classification\n",
        "        Architecture:\n",
        "        - 4 convolutional layers with kernel_size=5, padding=2, and max pooling (kernel_size=2)\n",
        "        - Channels: 1 → 16 → 32 → 64 → 128\n",
        "        - 4 fully connected layers: flattened_size → 1024 → 256 → 32 → 4 classes\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Convolutional layers with padding and max pooling\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        # Calculate flattened size after convolutions and pooling\n",
        "        # Each max pooling layer reduces dimensions by half\n",
        "        self.flattened_size = 128 * (input_height//16) * (input_width//16)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(self.flattened_size, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 256)\n",
        "        self.fc3 = nn.Linear(256, 32)\n",
        "        self.fc4 = nn.Linear(32, 4)  # 4 output classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "\n",
        "        x = x.view(-1, self.flattened_size)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOg1-DIZKLSf"
      },
      "outputs": [],
      "source": [
        "# Initialize model, criterion and optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_cnn_p = CNNP(input_height=input_height, input_width=input_width).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_cnn_p.parameters(), lr=0.002)\n",
        "\n",
        "# Train with validation\n",
        "best_model_p = train_with_validation(model_cnn_p, train_mel_loader, val_mel_loader, criterion, optimizer, num_epochs=30)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_f1, test_acc, test_cm = evaluate_model_device(best_model_p, test_mel_loader, criterion, device)\n",
        "\n",
        "print(\"\\nFinal Test Results with Best Model:\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test F1 Score (macro): {test_f1:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(test_cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X0IqigOw4cm"
      },
      "source": [
        "## Βήμα 5: Activation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSE_9-DXJuBT"
      },
      "outputs": [],
      "source": [
        "class CNNRELU(nn.Module):\n",
        "    def __init__(self, input_height, input_width):\n",
        "        super().__init__()\n",
        "\n",
        "        # Convolutional layers with ReLU and max pooling\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),  # Added ReLU\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),  # Added ReLU\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),  # Added ReLU\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),  # Added ReLU\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        # Calculate flattened size\n",
        "        self.flattened_size = 128 * (input_height//16) * (input_width//16)\n",
        "\n",
        "        # Fully connected layers with ReLU\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(self.flattened_size, 1024),\n",
        "            nn.ReLU()  # Added ReLU\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.ReLU()  # Added ReLU\n",
        "        )\n",
        "        self.fc3 = nn.Sequential(\n",
        "            nn.Linear(256, 32),\n",
        "            nn.ReLU()  # Added ReLU\n",
        "        )\n",
        "        self.fc4 = nn.Linear(32, 4)  # No ReLU before final output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "\n",
        "        x = x.view(-1, self.flattened_size)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QhOORaIKZ_8"
      },
      "outputs": [],
      "source": [
        "# Initialize model, criterion and optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_cnn_relu = CNNRELU(input_height=input_height, input_width=input_width).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_cnn_relu.parameters(), lr=0.002)\n",
        "\n",
        "# Train with validation\n",
        "best_model_relu = train_with_validation(model_cnn_relu, train_mel_loader, val_mel_loader, criterion, optimizer, num_epochs=30)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_f1, test_acc, test_cm = evaluate_model_device(best_model_relu, test_mel_loader, criterion, device)\n",
        "\n",
        "print(\"\\nFinal Test Results with Best Model:\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test F1 Score (macro): {test_f1:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(test_cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gySWRIrVxBPG"
      },
      "source": [
        "# Ερώτημα 3: Improving Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etvgJFWXxE2m"
      },
      "source": [
        "## Βήμα 1: Reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0qA9xIFLi0X"
      },
      "source": [
        "Δεν υλοποιήθηκε"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8xD1lVgxHlK"
      },
      "source": [
        "## Βήμα 2: Αλγόριθμοι βελτιστοποίησης"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SX36dUR2EVwB"
      },
      "outputs": [],
      "source": [
        "# Ορισμός των optimizers\n",
        "optimizers = {\n",
        "    'SGD': optim.SGD(model_cnn_relu.parameters(), lr=0.002),\n",
        "    'Adam': optim.Adam(model_cnn_relu.parameters(), lr=0.002),\n",
        "    'RMSprop': optim.RMSprop(model_cnn_relu.parameters(), lr=0.002),\n",
        "    'Adagrad': optim.Adagrad(model_cnn_relu.parameters(), lr=0.002),\n",
        "    'Adadelta': optim.Adadelta(model_cnn_relu.parameters(), lr=0.002),\n",
        "    'Adamax': optim.Adamax(model_cnn_relu.parameters(), lr=0.002),\n",
        "    'Nadam': optim.NAdam(model_cnn_relu.parameters(), lr=0.002),\n",
        "    'Adafactor': optim.Adafactor(model_cnn_relu.parameters(), lr=0.002),\n",
        "    'Rprop': optim.Rprop(model_cnn_relu.parameters(), lr=0.002),\n",
        "    'ASGD': optim.ASGD(model_cnn_relu.parameters(), lr=0.002)\n",
        "}\n",
        "\n",
        "# Αποθήκευση αποτελεσμάτων\n",
        "results = {'Optimizer': [], 'Accuracy': [], 'F1 Score': []}\n",
        "\n",
        "# Εκπαίδευση και αξιολόγηση για κάθε optimizer\n",
        "for opt_name, optimizer in optimizers.items():\n",
        "    print(f\"\\nTraining with {opt_name}...\")\n",
        "\n",
        "    # Initialize model, criterion and optimizer\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model_opt = CNNRELU(input_height=input_height, input_width=input_width).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Train with validation\n",
        "    best_model_opt = train_with_validation(model_opt, train_mel_loader, val_mel_loader, criterion, optimizer, 10)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_loss, test_f1, test_acc, test_cm = evaluate_model_device(best_model_opt, test_mel_loader, criterion, device)\n",
        "\n",
        "    results['Optimizer'].append(opt_name)\n",
        "    results['Accuracy'].append(test_acc)\n",
        "    results['F1 Score'].append(test_f1)\n",
        "\n",
        "# Εμφάνιση αποτελεσμάτων\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nResults:\")\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8XBETp8xKTx"
      },
      "source": [
        "## Βήμα 3: Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6dTn_ggKDk_"
      },
      "outputs": [],
      "source": [
        "class CNNBatchNorm(nn.Module):\n",
        "    def __init__(self, input_height, input_width, out_dim=4):\n",
        "        super(CNNBatchNorm, self).__init__()\n",
        "\n",
        "        # Convolutional layers with Batch Normalization and ReLU\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(16),  # Batch Normalization\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(32),  # Batch Normalization\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(64),  # Batch Normalization\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(128),  # Batch Normalization\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        # Calculate flattened size after convolutions and pooling\n",
        "        self.flattened_size = 128 * (input_height // 16) * (input_width // 16)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(self.flattened_size, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 256)\n",
        "        self.fc3 = nn.Linear(256, 32)\n",
        "        self.fc4 = nn.Linear(32, out_dim)  # 4 output classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "\n",
        "        x = x.view(-1, self.flattened_size)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txLV3C5bKPEQ"
      },
      "outputs": [],
      "source": [
        "# Initialize model, criterion and optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_cnn_batch = CNNBatchNorm(input_height=input_height, input_width=input_width).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_cnn_batch.parameters(), lr=0.002)\n",
        "\n",
        "# Train with validation\n",
        "best_model_batch = train_with_validation(model_cnn_batch, train_mel_loader, val_mel_loader, criterion, optimizer, num_epochs=30)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_f1, test_acc, test_cm = evaluate_model_device(best_model_batch, test_mel_loader, criterion, device)\n",
        "\n",
        "print(\"\\nFinal Test Results with Best Model:\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test F1 Score (macro): {test_f1:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(test_cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMsFYFj0xPi8"
      },
      "source": [
        "## Βήμα 4: Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "92rFRLruKwJg",
        "outputId": "d1631470-44f2-4c39-d255-8575b31d02ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30:\n",
            "Train Loss: 1.3831 | Val Loss: 1.3864 | Val F1: 0.1312\n",
            "Epoch 2/30:\n",
            "Train Loss: 1.3830 | Val Loss: 1.3864 | Val F1: 0.1318\n",
            "Epoch 3/30:\n",
            "Train Loss: 1.3833 | Val Loss: 1.3862 | Val F1: 0.1344\n",
            "Epoch 4/30:\n",
            "Train Loss: 1.3833 | Val Loss: 1.3858 | Val F1: 0.1323\n",
            "Epoch 5/30:\n",
            "Train Loss: 1.3830 | Val Loss: 1.3871 | Val F1: 0.1324\n",
            "Epoch 6/30:\n",
            "Train Loss: 1.3833 | Val Loss: 1.3857 | Val F1: 0.1365\n",
            "Epoch 7/30:\n",
            "Train Loss: 1.3832 | Val Loss: 1.3852 | Val F1: 0.1365\n",
            "Epoch 8/30:\n",
            "Train Loss: 1.3833 | Val Loss: 1.3857 | Val F1: 0.1339\n",
            "Epoch 9/30:\n",
            "Train Loss: 1.3828 | Val Loss: 1.3846 | Val F1: 0.1346\n",
            "Epoch 10/30:\n",
            "Train Loss: 1.3830 | Val Loss: 1.3864 | Val F1: 0.1318\n",
            "Epoch 11/30:\n",
            "Train Loss: 1.3828 | Val Loss: 1.3848 | Val F1: 0.1366\n",
            "Epoch 12/30:\n",
            "Train Loss: 1.3829 | Val Loss: 1.3869 | Val F1: 0.1317\n",
            "Epoch 13/30:\n",
            "Train Loss: 1.3833 | Val Loss: 1.3861 | Val F1: 0.1323\n",
            "Epoch 14/30:\n",
            "Train Loss: 1.3831 | Val Loss: 1.3864 | Val F1: 0.1318\n",
            "Epoch 15/30:\n",
            "Train Loss: 1.3831 | Val Loss: 1.3862 | Val F1: 0.1318\n",
            "Epoch 16/30:\n",
            "Train Loss: 1.3835 | Val Loss: 1.3855 | Val F1: 0.1365\n",
            "Early stopping triggered\n",
            "Epoch 1/60:\n",
            "Train Loss: 1.3961 | Val Loss: 1.3969 | Val F1: 0.1856\n",
            "Epoch 2/60:\n",
            "Train Loss: 1.3959 | Val Loss: 1.3965 | Val F1: 0.1856\n",
            "Epoch 3/60:\n",
            "Train Loss: 1.3963 | Val Loss: 1.3966 | Val F1: 0.1870\n",
            "Epoch 4/60:\n",
            "Train Loss: 1.3962 | Val Loss: 1.3970 | Val F1: 0.1831\n",
            "Epoch 5/60:\n",
            "Train Loss: 1.3962 | Val Loss: 1.3965 | Val F1: 0.1846\n",
            "Epoch 6/60:\n",
            "Train Loss: 1.3960 | Val Loss: 1.3967 | Val F1: 0.1846\n",
            "Epoch 7/60:\n",
            "Train Loss: 1.3959 | Val Loss: 1.3969 | Val F1: 0.1836\n",
            "Epoch 8/60:\n",
            "Train Loss: 1.3964 | Val Loss: 1.3966 | Val F1: 0.1861\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "CNNBatchNorm.__init__() got an unexpected keyword argument 'dropout_prob'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-e5ef180ace2b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Εκπαίδευση με και τα δύο (weight_decay και dropout)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0moptimizer_both\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_cnn_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.002\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel_both\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNNBatchNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mmodel_both\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_with_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_both\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mel_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mel_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_both\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: CNNBatchNorm.__init__() got an unexpected keyword argument 'dropout_prob'"
          ]
        }
      ],
      "source": [
        "# Εκπαίδευση με μόνο weight_decay\n",
        "optimizer_wd = optim.SGD(model_cnn_batch.parameters(), lr=0.002, weight_decay=1e-4)\n",
        "model_wd = CNNBatchNorm(input_height=input_height, input_width=input_width).to(device)\n",
        "model_wd = train_with_validation(model_wd, train_mel_loader, val_mel_loader, criterion, optimizer_wd, num_epochs=30)\n",
        "\n",
        "# Εκπαίδευση με μόνο dropout\n",
        "optimizer_dropout = optim.SGD(model_cnn_batch.parameters(), lr=0.002)\n",
        "model_dropout = CNNBatchNorm(input_height=input_height, input_width=input_width).to(device)\n",
        "model_dropout = train_with_validation(model_dropout, train_mel_loader, val_mel_loader, criterion, optimizer_dropout, num_epochs=60)\n",
        "\n",
        "# Εκπαίδευση με και τα δύο (weight_decay και dropout)\n",
        "optimizer_both = optim.SGD(model_cnn_batch.parameters(), lr=0.002, weight_decay=1e-4)\n",
        "model_both = CNNBatchNorm(input_height=input_height, input_width=input_width, dropout_prob=0.5).to(device)\n",
        "model_both = train_with_validation(model_both, train_mel_loader, val_mel_loader, criterion, optimizer_both, num_epochs=60)\n",
        "\n",
        "# Αξιολόγηση για όλα τα μοντέλα\n",
        "test_loss_wd, test_f1_wd, test_acc_wd, test_cm_wd = evaluate_model_device(model_wd, test_mel_loader, criterion, device)\n",
        "test_loss_dropout, test_f1_dropout, test_acc_dropout, test_cm_dropout = evaluate_model_device(model_dropout, test_mel_loader, criterion, device)\n",
        "test_loss_both, test_f1_both, test_acc_both, test_cm_both = evaluate_model_device(model_both, test_mel_loader, criterion, device)\n",
        "\n",
        "# Εμφάνιση αποτελεσμάτων\n",
        "print(\"\\nTest Results (Weight Decay Only):\")\n",
        "print(f\"Loss: {test_loss_wd:.4f}\")\n",
        "print(f\"F1 Score: {test_f1_wd:.4f}\")\n",
        "print(f\"Accuracy: {test_acc_wd:.4f}\")\n",
        "\n",
        "print(\"\\nTest Results (Dropout Only):\")\n",
        "print(f\"Loss: {test_loss_dropout:.4f}\")\n",
        "print(f\"F1 Score: {test_f1_dropout:.4f}\")\n",
        "print(f\"Accuracy: {test_acc_dropout:.4f}\")\n",
        "\n",
        "print(\"\\nTest Results (Both Weight Decay and Dropout):\")\n",
        "print(f\"Loss: {test_loss_both:.4f}\")\n",
        "print(f\"F1 Score: {test_f1_both:.4f}\")\n",
        "print(f\"Accuracy: {test_acc_both:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A8Bcg1CxSnn"
      },
      "source": [
        "# Ερώτημα 4: Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhvGtV9kxWQN"
      },
      "source": [
        "## Βήμα 1: Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq_XOPgRSfoj"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, dataloader, device=None):\n",
        "    \"\"\"\n",
        "    Παράγει προβλέψεις από ένα εκπαιδευμένο μοντέλο CNN για τα δεδομένα ενός dataloader.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): Το εκπαιδευμένο μοντέλο CNN\n",
        "        dataloader (torch.utils.data.DataLoader): Ο dataloader με τα δεδομένα (shuffle=False)\n",
        "        device (torch.device, optional): Σε ποια συσκευή να τρέξει (π.χ. 'cuda' ή 'cpu').\n",
        "                                        Αν None, θα χρησιμοποιηθεί το ίδιο device με το μοντέλο.\n",
        "\n",
        "    Returns:\n",
        "        list: Λίστα με όλες τις προβλέψεις του μοντέλου (στη σειρά που δόθηκαν)\n",
        "    \"\"\"\n",
        "    # Βάζουμε το μοντέλο σε evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Αν δεν δοθεί device, χρησιμοποιούμε αυτό του μοντέλου\n",
        "    if device is None:\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    # Απενεργοποιούμε τον υπολογισμό gradients για αποδοτικότητα\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in dataloader:  # αγνοούμε τα labels (αν υπάρχουν)\n",
        "            # Μεταφέρουμε τα δεδομένα στο σωστό device\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            # Κάνουμε την πρόβλεψη\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Παίρνουμε την κλάση με την υψηλότερη πιθανότητα\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # Μεταφέρουμε τις προβλέψεις σε CPU και τις προσθέτουμε στη λίστα\n",
        "            predictions.extend(preds.cpu().numpy().tolist())\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W0xileSxZHi"
      },
      "source": [
        "## Βήμα 2: Κατέβασμα μουσικής από το youtube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUrL_0C1R5O8"
      },
      "outputs": [],
      "source": [
        "def download_youtube(youtube_url, output_path=\"output.wav\"):\n",
        "    try:\n",
        "        # Download audio using yt_dlp\n",
        "        ydl_opts = {\n",
        "            'format': 'bestaudio/best',\n",
        "            'outtmpl': 'downloaded_audio.%(ext)s',\n",
        "            'postprocessors': [{\n",
        "                'key': 'FFmpegExtractAudio',\n",
        "                'preferredcodec': 'mp3',\n",
        "                'preferredquality': '192',\n",
        "            }],\n",
        "        }\n",
        "\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            ydl.download([youtube_url])\n",
        "\n",
        "        audio_file = \"downloaded_audio.mp3\"\n",
        "        audio = AudioSegment.from_file(audio_file)\n",
        "        audio.export(output_path, format=\"wav\")\n",
        "\n",
        "        os.remove(audio_file)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Download failed: {e}\")\n",
        "        raise\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUohBTn5R8Hm"
      },
      "outputs": [],
      "source": [
        "window_length = (50 * 1e-3)\n",
        "hop_length = (50 * 1e-3)\n",
        "mel_time_size = 21\n",
        "\n",
        "def load_wav(filename):\n",
        "    \"\"\"Rea audio file and return audio signal and sampling frequency\"\"\"\n",
        "    if not os.path.exists(filename):\n",
        "        raise FileNotFoundError\n",
        "    # Load file using librosa\n",
        "    x, fs = librosa.load(filename, sr=None)\n",
        "    return x, fs\n",
        "\n",
        "\n",
        "def melspectrogram(x=None, fs=None, n_fft=None, hop_length=None,\n",
        "                   fuse=False):\n",
        "    \"\"\"Returns a mel spectrogram.\"\"\"\n",
        "\n",
        "    if x is None:\n",
        "        return None\n",
        "    # Set some values\n",
        "    if n_fft is None:\n",
        "        n_fft = int(window_length * fs)\n",
        "    if hop_length is None:\n",
        "        hop_length = int(hop_length * fs)\n",
        "    # Get spectrogram\n",
        "    spectrogram = librosa.feature.melspectrogram(y=x, sr=fs, n_fft=n_fft,\n",
        "                                                 hop_length=hop_length)\n",
        "    # Convert to MEL-Scale\n",
        "    spectrogram_dB = librosa.power_to_db(spectrogram, ref=np.max)  # (n_mel,t)\n",
        "\n",
        "    if fuse:\n",
        "        chroma = librosa.feature.chroma_stft(y=x, sr=fs, n_fft=n_fft,\n",
        "                                             hop_length=hop_length)\n",
        "        chroma_dB = librosa.power_to_db(chroma)\n",
        "        out = np.concatenate((spectrogram_dB.T, chroma_dB.T), axis=1)\n",
        "    else:\n",
        "        # Transpose to return (time,n_mel)\n",
        "        out = spectrogram_dB.T\n",
        "    return out\n",
        "\n",
        "\n",
        "def get_melgrams(file):\n",
        "    signal, fs = load_wav(file)\n",
        "\n",
        "    segment_length = int((mel_time_size - 1) * window_length * fs)\n",
        "    sequence_length = signal.shape[0]\n",
        "    progress = 0\n",
        "    segments = []\n",
        "    while progress < sequence_length:\n",
        "        if progress + segment_length > sequence_length:\n",
        "            fill_data = sequence_length - progress\n",
        "            empty_data = segment_length - fill_data\n",
        "            feature = melspectrogram(\n",
        "                np.pad(signal[progress:], (0, empty_data), 'constant'),\n",
        "                fs=fs, n_fft=int(window_length * fs), hop_length=int(hop_length * fs))\n",
        "            segments.append(feature)\n",
        "        else:\n",
        "            feature = melspectrogram(\n",
        "                signal[progress:progress + segment_length],\n",
        "                fs=fs, n_fft=int(window_length * fs), hop_length=int(hop_length * fs))\n",
        "\n",
        "            segments.append(feature)\n",
        "        progress += segment_length\n",
        "\n",
        "    return segments\n",
        "\n",
        "\n",
        "def youtube_to_melgram(url):\n",
        "    download_youtube(url)\n",
        "    melgrams = get_melgrams(\"output.wav\")\n",
        "    np.save(\"youtube_melgrams.npy\", melgrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WphWpK02SYQ7"
      },
      "outputs": [],
      "source": [
        "youtube_to_melgram('https://www.youtube.com/watch?v=9E6b3swbnWg')\n",
        "youtube_to_melgram('https://www.youtube.com/watch?v=EDwb9jOVRtU')\n",
        "youtube_to_melgram('https://www.youtube.com/watch?v=OMaycNcPsHI')\n",
        "youtube_to_melgram('https://www.youtube.com/watch?v=l45f28PzfCI')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HrG4C1axb6Y"
      },
      "source": [
        "## Βήμα 3: Προβλέψεις"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dkouvcBUJCH"
      },
      "outputs": [],
      "source": [
        "def plot_predictions_heatmap(predictions, class_names, title=\"Predictions over Time\"):\n",
        "    \"\"\"\n",
        "    Plots a heatmap of predictions over time (per second).\n",
        "\n",
        "    Args:\n",
        "        predictions (list): List of predicted class indices (0 to num_classes-1).\n",
        "        class_names (list): List of class names (e.g., ['rock', 'pop', ...]).\n",
        "        title (str): Title of the plot.\n",
        "    \"\"\"\n",
        "    num_timesteps = len(predictions)\n",
        "    num_classes = len(class_names)\n",
        "\n",
        "    # Create heatmap data (one-hot encoding)\n",
        "    heatmap_data = np.zeros((num_classes, num_timesteps))\n",
        "    for t, pred in enumerate(predictions):\n",
        "        heatmap_data[pred, t] = 1\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    sns.heatmap(\n",
        "        heatmap_data,\n",
        "        cmap=\"YlOrRd\",\n",
        "        yticklabels=class_names,\n",
        "        xticklabels=10,  # Show every 10th timestep for clarity\n",
        "        cbar_kws={'label': 'Prediction (1=yes, 0=no)'}\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Time (seconds)\")\n",
        "    plt.ylabel(\"Music Genre\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98LThtbOUL_2"
      },
      "outputs": [],
      "source": [
        "def evaluate_video_predictions(predictions, test_accuracy, class_names):\n",
        "    \"\"\"\n",
        "    Evaluates video predictions and compares with test set accuracy.\n",
        "\n",
        "    Args:\n",
        "        predictions (list): Predicted classes for each timestep.\n",
        "        test_accuracy (float): Accuracy on the test set (0.0 to 1.0).\n",
        "        class_names (list): List of class names.\n",
        "    \"\"\"\n",
        "    # Check if predictions list is empty\n",
        "    if not predictions:\n",
        "        print(\"No predictions available for evaluation.\")\n",
        "        return\n",
        "\n",
        "    # Calculate majority class and consistency using mode result object\n",
        "    # .mode attribute returns the modal value(s)\n",
        "    # .count attribute returns the count(s) of the modal value(s)\n",
        "    mode_result = mode(predictions)\n",
        "    majority_class = int(mode_result.mode) # Access the mode value using .mode attribute\n",
        "    consistency = np.mean(np.array(predictions) == majority_class)\n",
        "\n",
        "    print(f\"\\nMajority class: {class_names[majority_class]}\")\n",
        "    print(f\"Consistency: {consistency:.2%}\")\n",
        "    print(f\"Test set accuracy: {test_accuracy:.2%}\")\n",
        "\n",
        "    # Compare with test accuracy\n",
        "    if consistency >= test_accuracy * 0.9:  # Allow 10% tolerance\n",
        "        print(\"Model performance is consistent with test set.\")\n",
        "    else:\n",
        "        print(\"Model performance differs significantly from test set.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjlzmtBeUNbh"
      },
      "outputs": [],
      "source": [
        "# Load your trained model and dataloaders\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = best_model_relu  # Your trained CNN model (from previous steps)\n",
        "test_accuracy = 0.68\n",
        "class_names = list(class_mapping.values())  # From your existing code\n",
        "\n",
        "# Load YouTube melgrams (replace with your actual data)\n",
        "youtube_melgrams = np.load(\"youtube_melgrams.npy\", allow_pickle=True)\n",
        "youtube_melgrams = torch.FloatTensor(youtube_melgrams).unsqueeze(1)  # Add channel dim\n",
        "\n",
        "# Create a DataLoader for inference (shuffle=False)\n",
        "youtube_dataset = TensorDataset(youtube_melgrams, torch.zeros(len(youtube_melgrams)))\n",
        "youtube_loader = DataLoader(youtube_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Get predictions\n",
        "predictions = get_predictions(model, youtube_loader, device)\n",
        "\n",
        "# Plot heatmap\n",
        "plot_predictions_heatmap(predictions, class_names, \"YouTube Video Predictions\")\n",
        "\n",
        "# Evaluate predictions\n",
        "evaluate_video_predictions(predictions, test_accuracy, class_names)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}